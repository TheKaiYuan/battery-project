{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "battery json data  convert to matrix and input to CNN for train，没有容量特征字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "from scipy.io import loadmat, whosmat\n",
    "import numpy as np\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "# 获取得到 json data of battery \n",
    "def json_load(dictonary):\n",
    "    with open(dictonary) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "'''\n",
    "cycle为行数，循环读取一行数据，对一行数据里面的voltage,current and temp求均值，并添加capacity features\n",
    "得到battery array\n",
    "'''\n",
    "def batteryData2array(battery_data):\n",
    "    vol_sum = 0\n",
    "    cur_sum = 0\n",
    "    temp_sum=0\n",
    "    battery_array = []\n",
    "    for cycle in battery_data.keys():\n",
    "        # list type \n",
    "        battery_vols = battery_data[cycle][\"voltage_battery\"]\n",
    "        battery_curs = battery_data[cycle][\"current_battery\"]\n",
    "        battery_temps=battery_data[cycle][\"temp_battery\"]\n",
    "        battery_capacity=battery_data[cycle][\"Capacity\"]\n",
    "        for battery_vol in battery_vols:\n",
    "            vol_sum += float(battery_vol)\n",
    "        for battery_cur in battery_curs:\n",
    "            cur_sum += float(battery_cur)\n",
    "        for battery_temp in battery_temps:\n",
    "            temp_sum += float(battery_temp)\n",
    "        #for voltage and current average  & battery_vols,battery_curs,battery_temps's len is equal\n",
    "        num = len(battery_vols)\n",
    "        vol_aver = vol_sum / num\n",
    "        cur_aver = cur_sum / num\n",
    "        temp_aver=temp_sum/num\n",
    "        vol_sum = 0\n",
    "        cur_sum = 0\n",
    "        temp_sum=0\n",
    "        battery_array.append([vol_aver,cur_aver,temp_aver])\n",
    "#         battery_array.append([vol_aver,cur_aver,battery_capacity[0]])\n",
    "    return battery_array\n",
    "\n",
    "# get battery's true soc\n",
    "def  getTrueSOC(battery_discharge_data):\n",
    "    batteryCapcity=2\n",
    "    soc_dict=[]\n",
    "    for cycle in battery_discharge_data.keys():\n",
    "        cycleCapacity=battery_discharge_data[cycle][\"Capacity\"]\n",
    "        soc_true=cycleCapacity[0]/batteryCapcity\n",
    "        soc_dict.append(soc_true)\n",
    "    return soc_dict\n",
    "\n",
    "# get battery's discgarge capacity\n",
    "def  getTrueCapacity(battery_discharge_data):\n",
    "    capacity=[]\n",
    "    for cycle in battery_discharge_data.keys():\n",
    "        cycleCapacity=battery_discharge_data[cycle][\"Capacity\"]\n",
    "        capacity.append(cycleCapacity[0])\n",
    "    return capacity\n",
    "\n",
    "'''\n",
    "获取得到 battery 's img matrix 图像矩阵  4维特征向量  vol_aver,cur_aver,temp_aver,battery_capacity\n",
    "返回battery discharge matrix and true soc array\n",
    "'''\n",
    "def batterydata2matrix(discharge):\n",
    "    #get the discharge and charge json data\n",
    "    discharge_data =json_load(discharge)\n",
    "    \n",
    "    #get the true soc data\n",
    "    true_soc=getTrueSOC(discharge_data)\n",
    "#   true_capacity=getTrueCapacity(discharge_data)\n",
    "    #get the battery discharge array data  convert to matrix\n",
    "    discharge_array = batteryData2array(discharge_data)\n",
    "    discharge_matrix = np.array(discharge_array)\n",
    "\n",
    "    discharge_matrix=preprocessing.scale(discharge_matrix)\n",
    "    return discharge_matrix,true_soc\n",
    "\n",
    "'''\n",
    "获取得到每一节电池数据的训练数据集\n",
    "得到train & test data\n",
    "'''\n",
    "def getTrainData(discharge_matrix,true_soc):     \n",
    "    #循环切片形成数据集\n",
    "    x=[];y=[]\n",
    "    x_train=[];y_train=[]\n",
    "    flag=0\n",
    "\n",
    "    # 获取得到x,y的数据集\n",
    "    for i in range(len(discharge_matrix.tolist())):\n",
    "        #首先截取长度为8的矩阵，叠加循环得到x\n",
    "        bataery_data_training=discharge_matrix[flag:flag+8,:]\n",
    "    #     print(bataery_data_training.shape)\n",
    "        if(len(bataery_data_training)<8):\n",
    "            break\n",
    "        else:\n",
    "            #进行转置，形成4*T的时间维度\n",
    "            x.append(bataery_data_training.T)\n",
    "        flag+=1\n",
    "    #0-》t时刻d x 预测得到t时刻的soc。为了对soc 进行预测，y矩阵为【7：】\n",
    "    y=true_soc[8:]\n",
    "    x=x[:len(x)-1]\n",
    "#     y=true_capacity[8:]\n",
    "\n",
    "    VALIDATION_SPLIT=0.9\n",
    "    x_validation_samples = int( VALIDATION_SPLIT* len(x))\n",
    "    y_validation_samples = int( VALIDATION_SPLIT*len(y))\n",
    "    \n",
    "    \n",
    "    # 获取得到训练集，验证集，比例9:1\n",
    "    x_train=np.array(x[:x_validation_samples])\n",
    "    x_test=np.array(x[x_validation_samples:])\n",
    "    y_train=np.array(y[:y_validation_samples])\n",
    "    y_test=np.array(y[y_validation_samples:])\n",
    "    print(\"x_train's shape is:\", x_train.shape,\"，y_train's shape is:\",y_train.shape)\n",
    "    print(\"x_test's shape is:\",x_test.shape,\"，y_test's shape is:\",y_test.shape)\n",
    "    return x_train,x_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train's shape is: (144, 3, 8) ，y_train's shape is: (144,)\n",
      "x_test's shape is: (16, 3, 8) ，y_test's shape is: (16,)\n",
      "x_train's shape is: (144, 3, 8) ，y_train's shape is: (144,)\n",
      "x_test's shape is: (16, 3, 8) ，y_test's shape is: (16,)\n",
      "x_train's shape is: (144, 3, 8) ，y_train's shape is: (144,)\n",
      "x_test's shape is: (16, 3, 8) ，y_test's shape is: (16,)\n",
      "(432, 3, 8) (48, 3, 8) (432,) (48,)\n"
     ]
    }
   ],
   "source": [
    "# 定义全局变量，将所有电池数据集集合起来，形成训练集，测试集\n",
    "# x_train_all,x_test_all,y_train_all,y_test_all=[],[],[],[]\n",
    "for i in range(5,8):\n",
    "    #/home/aqts/yangHong/battery experiment/jsonData\n",
    "    discharge=r'/home/aqts/yangHong/battery experiment/jsonData/B000'+str(i)+'_discharge.json'\n",
    "    discharge_matrix,true_soc=batterydata2matrix(discharge)\n",
    "    train_x,test_x,train_y,test_y=getTrainData(discharge_matrix,true_soc)\n",
    "    # start 开始时先赋值给x_train_all，x_test_all，y_train_all，y_test_all\n",
    "    if i==5:\n",
    "        x_train_all=train_x\n",
    "        x_test_all=test_x\n",
    "        y_train_all=train_y\n",
    "        y_test_all=test_y\n",
    "    else:\n",
    "        x_train_all=np.concatenate((x_train_all,train_x),axis=0)\n",
    "        x_test_all=np.concatenate((x_test_all,test_x),axis=0)\n",
    "        y_train_all=np.concatenate((y_train_all,train_y),axis=0)\n",
    "        y_test_all=np.concatenate((y_test_all,test_y),axis=0)\n",
    "print(np.array(x_train_all).shape,np.array(x_test_all).shape,np.array(y_train_all).shape,np.array(y_test_all).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写一个LossHistory类，保存loss和mae\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D,Convolution2D\n",
    "from keras import backend as k\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('mean_absolute_error'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_mean_absolute_error'))\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('mean_absolute_error'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_mean_absolute_error'))\n",
    "\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        plt.figure()\n",
    "        # mae\n",
    "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train mae')\n",
    "        # loss\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # val_mean_absolute_error\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val mean absolute error')\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('mae-loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?)\n",
      "Tensor(\"reshape_11/Reshape:0\", shape=(?, ?, 2), dtype=float32)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_62 (InputLayer)        (None, 3, 8, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_114 (Conv2D)          (None, 3, 8, 64)          192       \n",
      "_________________________________________________________________\n",
      "conv2d_115 (Conv2D)          (None, 3, 8, 128)         32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 3, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_116 (Conv2D)          (None, 3, 4, 32)          8224      \n",
      "_________________________________________________________________\n",
      "conv2d_117 (Conv2D)          (None, 3, 4, 64)          12352     \n",
      "_________________________________________________________________\n",
      "flatten_28 (Flatten)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "reshape_11 (Reshape)         (None, 384, 2)            0         \n",
      "_________________________________________________________________\n",
      "gru_40 (GRU)                 (None, 384, 16)           912       \n",
      "_________________________________________________________________\n",
      "gru_41 (GRU)                 (None, 64)                15552     \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 71,185\n",
      "Trainable params: 71,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 302 samples, validate on 130 samples\n",
      "Epoch 1/128\n",
      "302/302 [==============================] - 29s 95ms/step - loss: 0.2843 - mean_absolute_error: 0.4597 - val_loss: 0.0784 - val_mean_absolute_error: 0.2194\n",
      "Epoch 2/128\n",
      "302/302 [==============================] - 22s 74ms/step - loss: 0.0801 - mean_absolute_error: 0.1821 - val_loss: 0.0374 - val_mean_absolute_error: 0.0866\n",
      "Epoch 3/128\n",
      "302/302 [==============================] - 23s 75ms/step - loss: 0.0689 - mean_absolute_error: 0.1629 - val_loss: 0.0330 - val_mean_absolute_error: 0.0667\n",
      "Epoch 4/128\n",
      "302/302 [==============================] - 22s 74ms/step - loss: 0.0638 - mean_absolute_error: 0.1533 - val_loss: 0.0336 - val_mean_absolute_error: 0.0692\n",
      "Epoch 5/128\n",
      "302/302 [==============================] - 23s 75ms/step - loss: 0.0590 - mean_absolute_error: 0.1433 - val_loss: 0.0333 - val_mean_absolute_error: 0.0683\n",
      "Epoch 6/128\n",
      "302/302 [==============================] - 23s 75ms/step - loss: 0.0574 - mean_absolute_error: 0.1421 - val_loss: 0.0355 - val_mean_absolute_error: 0.0778\n",
      "Epoch 7/128\n",
      "302/302 [==============================] - 23s 76ms/step - loss: 0.0615 - mean_absolute_error: 0.1494 - val_loss: 0.0341 - val_mean_absolute_error: 0.0715\n",
      "Epoch 8/128\n",
      "302/302 [==============================] - 23s 75ms/step - loss: 0.0572 - mean_absolute_error: 0.1396 - val_loss: 0.0329 - val_mean_absolute_error: 0.0669\n",
      "Epoch 9/128\n",
      "302/302 [==============================] - 23s 75ms/step - loss: 0.0548 - mean_absolute_error: 0.1345 - val_loss: 0.0331 - val_mean_absolute_error: 0.0675\n",
      "Epoch 10/128\n",
      "302/302 [==============================] - 23s 76ms/step - loss: 0.0517 - mean_absolute_error: 0.1270 - val_loss: 0.0329 - val_mean_absolute_error: 0.0670\n",
      "Epoch 11/128\n",
      "302/302 [==============================] - 23s 76ms/step - loss: 0.0531 - mean_absolute_error: 0.1331 - val_loss: 0.0326 - val_mean_absolute_error: 0.0659\n",
      "Epoch 12/128\n",
      "302/302 [==============================] - 23s 76ms/step - loss: 0.0553 - mean_absolute_error: 0.1414 - val_loss: 0.0327 - val_mean_absolute_error: 0.0662\n",
      "Epoch 13/128\n",
      "302/302 [==============================] - 23s 76ms/step - loss: 0.0481 - mean_absolute_error: 0.1210 - val_loss: 0.0327 - val_mean_absolute_error: 0.0662\n",
      "Epoch 14/128\n",
      "302/302 [==============================] - 23s 77ms/step - loss: 0.0474 - mean_absolute_error: 0.1187 - val_loss: 0.0329 - val_mean_absolute_error: 0.0671\n",
      "Epoch 15/128\n",
      "302/302 [==============================] - 23s 76ms/step - loss: 0.0476 - mean_absolute_error: 0.1231 - val_loss: 0.0322 - val_mean_absolute_error: 0.0646\n",
      "Epoch 16/128\n",
      "302/302 [==============================] - 23s 76ms/step - loss: 0.0490 - mean_absolute_error: 0.1230 - val_loss: 0.0328 - val_mean_absolute_error: 0.0670\n",
      "Epoch 17/128\n",
      "302/302 [==============================] - 22s 74ms/step - loss: 0.0475 - mean_absolute_error: 0.1191 - val_loss: 0.0328 - val_mean_absolute_error: 0.0672\n",
      "Epoch 18/128\n",
      "160/302 [==============>...............] - ETA: 8s - loss: 0.0490 - mean_absolute_error: 0.1210"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-c9e4ef03f490>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    103\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m           verbose=1,validation_split=0.3,callbacks=[history])\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0mmse_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmae_score\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#使用cnn训练的模型\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D,Convolution2D,GRU,LSTM,Bidirectional,RepeatVector,concatenate\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import tensorflow as tf\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.core import Reshape\n",
    "# 引用GPU命令\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "num=1\n",
    "img_rows, img_cols =x_train_all.shape[1],x_train_all.shape[2]\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    x_train_all = x_train_all.reshape(x_train_all.shape[0], 1, img_rows, img_cols)\n",
    "    x_test_all = x_test_all.reshape(x_test_all.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train_all = x_train_all.reshape(x_train_all.shape[0], img_rows, img_cols, 1)\n",
    "    x_test_all= x_test_all.reshape(x_test_all.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# if K.image_dim_ordering() == 'th':\n",
    "#     x_train_all = x_train_all.reshape(x_train_all.shape[0], img_rows, img_cols)\n",
    "#     x_test_all = x_test_all.reshape(x_test_all.shape[0], img_rows, img_cols)\n",
    "#     input_shape = (1, img_rows, img_cols)\n",
    "# else:\n",
    "#     x_train_all = x_train_all.reshape(x_train_all.shape[0], img_rows, img_cols)\n",
    "#     x_test_all= x_test_all.reshape(x_test_all.shape[0], img_rows, img_cols)\n",
    "#     input_shape = (img_rows, img_cols, 1)\n",
    "    \n",
    "# keras 函数时搭建cnn模型\n",
    "# model building\n",
    "nb_lstm_outputs =16  #神经元个数\n",
    "main_input_cnn= Input(shape=input_shape, dtype='float32')\n",
    "# print(main_input_cnn.shape)\n",
    "# main_input_rnn = Input(shape=(img_rows, img_cols), dtype='float32')\n",
    "# print(main_input_rnn.shape)\n",
    "# CNN model \n",
    "cnn=Conv2D(64, kernel_size=(1,2),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape,padding=\"same\")(main_input_cnn)\n",
    "cnn=Conv2D(128, (2,2), activation='relu',padding=\"same\")(cnn)\n",
    "cnn=MaxPooling2D(pool_size=(1, 2))(cnn)\n",
    "# cnn=Dropout(0.20)(cnn)\n",
    "\n",
    "cnn=Conv2D(32, (2,1), activation='relu',padding=\"same\")(cnn)\n",
    "\n",
    "cnn=Conv2D(64, (3,2), activation='relu',padding=\"same\")(cnn)\n",
    "# print(\"cnn's first shape\",cnn.shape)\n",
    "# cnn=MaxPooling2D(pool_size=(2,2))(cnn)\n",
    "# cnn=Reshape((cnn[1]*cnn[2], cnn[3]))\n",
    "# print(\"cnn's reshae shape\",cnn)\n",
    "cnn=Flatten()(cnn)\n",
    "# print(\"cnn's second shape\",cnn.shape)\n",
    "# cnn=Dense(16, activation='relu',kernel_regularizer=regularizers.l2(0.001))(cnn)\n",
    "# cnn=Dropout(0.20)(cnn)\n",
    "# cnn=Dense(16, activation='relu')(cnn)\n",
    "# cnn=Dropout(0.20)(cnn)\n",
    "print(cnn.shape)\n",
    "# RNN model\n",
    "gru=Reshape((-1,2))(cnn)\n",
    "print(gru)\n",
    "rnn=GRU(units=nb_lstm_outputs,dropout=0.20, recurrent_dropout=0.10,return_sequences=True)(gru)\n",
    "rnn=GRU(64, dropout=0.20,recurrent_dropout=0.10)(rnn)\n",
    "rnn=Dropout(0.20)(rnn)\n",
    "rnn=Dense(16, activation='relu',kernel_regularizer=regularizers.l2(0.001))(rnn)\n",
    "rnn=Dropout(0.20)(rnn)\n",
    "\n",
    "# rnn=Dense(16, activation='relu')(rnn)\n",
    "# rnn=Dropout(0.20)(rnn)\n",
    "\n",
    "\n",
    "# con = concatenate([cnn,rnn], axis=-1)\n",
    "main_output = Dense(num)(rnn)\n",
    "# model = Model(inputs = [main_input_cnn,main_input_rnn], outputs = main_output)\n",
    "model = Model(inputs = [main_input_cnn], outputs = main_output)\n",
    "sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='mse', optimizer=\"sgd\", metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "batch_size =16\n",
    "num_epoch = 128\n",
    "\n",
    "\n",
    "model.summary()\n",
    "# decay_rate = learning_rate / num_epoch\n",
    "# momentum = 0.8\n",
    "# sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "\n",
    "# model.compile(loss='mse', optimizer=sgd, metrics=[\"mae\"])\n",
    "#model training\n",
    "history = LossHistory()\n",
    "model.fit(x_train_all, y_train_all,\n",
    "          batch_size=batch_size,\n",
    "          epochs=num_epoch,\n",
    "          verbose=1,validation_split=0.3,callbacks=[history])\n",
    "\n",
    "mse_score,mae_score= model.evaluate(x_test_all, y_test_all, verbose=0)\n",
    "print('Test MSE Score:%.3f %%'% (mse_score*100)) \n",
    "print('Test MAE Score:%.3f %%'% (mae_score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于序贯(Sequential)编程方式的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-2a7f56d0e73c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m           verbose=1,validation_split=0.3,callbacks=[history])\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0mmse_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmae_score\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0;31m# to match the value shapes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_set_inputs\u001b[0;34m(self, inputs, outputs, training)\u001b[0m\n\u001b[1;32m    587\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                                          \u001b[0;34m'You can build it manually via: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[0;32m--> 431\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mchild_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#使用cnn训练的模型\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D,Convolution2D,GRU,LSTM,Bidirectional,RepeatVector\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import tensorflow as tf\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "# 引用GPU命令\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "num=1\n",
    "time_steps=2\n",
    "\n",
    "# img_rows, img_cols =x_train_all.shape[1],x_train_all.shape[2]\n",
    "# 维度转换\n",
    "img_rows, img_cols =6,2\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    x_train_all = x_train_all.reshape(x_train_all.shape[0],time_steps, 1, img_rows, img_cols)\n",
    "    x_test_all = x_test_all.reshape(x_test_all.shape[0],time_steps, 1, img_rows, img_cols)\n",
    "    input_shape = (time_steps,1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train_all = x_train_all.reshape(x_train_all.shape[0], time_steps,img_rows, img_cols, 1)\n",
    "    x_test_all= x_test_all.reshape(x_test_all.shape[0],time_steps, img_rows, img_cols, 1)\n",
    "    input_shape = (time_steps,img_rows, img_cols, 1)\n",
    "    \n",
    "# lstm_layer=(None, input_shape[0], input_shape[1], input_shape[2])\n",
    "# print(x_train.shape,x_test.shape)\n",
    "# print(y_train.shape)\n",
    "# keras序贯模型搭建CNN\n",
    "# model building\n",
    "model = Sequential()\n",
    "#convolutional layer with rectified linear unit activation\n",
    "model.add(TimeDistributed(Conv2D(64, kernel_size=(1,2),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape,padding=\"same\")))\n",
    "#32 convolution filters used each of size 3x3\n",
    "#again\n",
    "model.add(TimeDistributed(Conv2D(128, (2,2), activation='relu',padding=\"same\")))\n",
    "#64 convolution filters used each of size 3x3\n",
    "#choose the best features via pooling\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(1, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(32, (2,1), activation='relu',padding=\"same\")))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(64, (2,2), activation='relu',padding=\"same\")))\n",
    "# model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(16))\n",
    "#fully connected to get all relevant data\n",
    "# model.add(TimeDistributed(Dropout(0.20)))\n",
    "# model.add(TimeDistributed(Dense(16, activation='relu',kernel_regularizer=regularizers.l2(0.001))))\n",
    "# model.add(TimeDistributed(Dropout(0.20)))\n",
    "\n",
    "# model.add(TimeDistributed(Dense(16, activation='relu')))\n",
    "# model.add(TimeDistributed(Dropout(0.20)))\n",
    "# model.add(Dense(num, activation='sigmoid'))\n",
    "# The last layer does not use the activation function\n",
    "model.add(TimeDistributed(Dense(num)))\n",
    "\n",
    "#Adaptive learning rate (adaDelta) is a popular form of gradient descent rivaled only by adam and adagrad\n",
    "#categorical ce since we have multiple classes (10) \n",
    "# model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "#               optimizer=keras.optimizers.Adadelta(),\n",
    "#               metrics=['accuracy'])\n",
    "#回归问题评价指标使用mae\n",
    "model.compile(loss='mse', optimizer=\"sgd\", metrics=[\"mae\"])\n",
    "\n",
    "batch_size =16\n",
    "num_epoch = 128\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# decay_rate = learning_rate / num_epoch\n",
    "# momentum = 0.8\n",
    "# sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "\n",
    "# model.compile(loss='mse', optimizer=sgd, metrics=[\"mae\"])\n",
    "#model training\n",
    "history = LossHistory()\n",
    "model.fit(x_train_all, y_train_all,\n",
    "          batch_size=batch_size,\n",
    "          epochs=num_epoch,\n",
    "          verbose=1,validation_split=0.3,callbacks=[history])\n",
    "model.summary()\n",
    "mse_score,mae_score= model.evaluate(x_test_all, y_test_all, verbose=0)\n",
    "print('Test MSE Score:%.3f %%'% (mse_score*100)) \n",
    "print('Test MAE Score:%.3f %%'% (mae_score*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 256, 3)\n",
      "Train on 302 samples, validate on 130 samples\n",
      "Epoch 1/32\n",
      "302/302 [==============================] - 18s 59ms/step - loss: 0.3909 - mean_absolute_error: 0.6083 - val_loss: 0.2060 - val_mean_absolute_error: 0.4488\n",
      "Epoch 2/32\n",
      "302/302 [==============================] - 14s 45ms/step - loss: 0.1190 - mean_absolute_error: 0.3189 - val_loss: 0.0572 - val_mean_absolute_error: 0.2293\n",
      "Epoch 3/32\n",
      "192/302 [==================>...........] - ETA: 4s - loss: 0.0504 - mean_absolute_error: 0.1838"
     ]
    }
   ],
   "source": [
    "#使用cnn训练的模型\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D,Convolution2D,GRU,LSTM,Bidirectional,RepeatVector\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import tensorflow as tf\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "# 引用GPU命令\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "num=1\n",
    "time_steps=2\n",
    "\n",
    "img_rows, img_cols =x_train_all.shape[1],x_train_all.shape[2]\n",
    "# 维度转换\n",
    "# img_rows, img_cols =6,2\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    x_train_all = x_train_all.reshape(x_train_all.shape[0], 1, img_rows, img_cols)\n",
    "    x_test_all = x_test_all.reshape(x_test_all.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train_all = x_train_all.reshape(x_train_all.shape[0],img_rows, img_cols, 1)\n",
    "    x_test_all= x_test_all.reshape(x_test_all.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    \n",
    "# lstm_layer=(None, input_shape[0], input_shape[1], input_shape[2])\n",
    "# print(x_train.shape,x_test.shape)\n",
    "# print(y_train.shape)\n",
    "# keras序贯模型搭建CNN\n",
    "# model building\n",
    "model = Sequential()\n",
    "#convolutional layer with rectified linear unit activation\n",
    "model.add(Conv2D(64, kernel_size=(1,2),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape,padding=\"same\"))\n",
    "#32 convolution filters used each of size 3x3\n",
    "#again\n",
    "model.add(Conv2D(128, (2,2), activation='relu',padding=\"same\"))\n",
    "#64 convolution filters used each of size 3x3\n",
    "#choose the best features via pooling\n",
    "model.add(MaxPooling2D(pool_size=(1, 2)))\n",
    "\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Conv2D(32, (2,1), activation='relu',padding=\"same\"))\n",
    "\n",
    "model.add(Conv2D(64, (2,2), activation='relu',padding=\"same\"))\n",
    "# model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Reshape((-1,3)))\n",
    "print(model.output_shape)\n",
    "model.add(GRU(32, return_sequences=True))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(GRU(16))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Dense(16,activation=\"relu\"))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Dense(num))\n",
    "\n",
    "#回归问题评价指标使用mae\n",
    "model.compile(loss='mse', optimizer=\"sgd\", metrics=[\"mae\"])\n",
    "\n",
    "batch_size =16\n",
    "num_epoch = 32\n",
    "\n",
    "#model training\n",
    "history = LossHistory()\n",
    "model.fit(x_train_all, y_train_all,\n",
    "          batch_size=batch_size,\n",
    "          epochs=num_epoch,\n",
    "          verbose=1,validation_split=0.3,callbacks=[history])\n",
    "model.summary()\n",
    "mse_score,mae_score= model.evaluate(x_test_all, y_test_all, verbose=0)\n",
    "print('Test MSE Score:%.3f %%'% (mse_score*100)) \n",
    "print('Test MAE Score:%.3f %%'% (mae_score*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#绘制mae-loss曲线\n",
    "history.loss_plot('epoch')\n",
    "# input_shape[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取得到需要用于预测的电池数据集B0018号电池"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test_008 data shape is (124, 3, 8)\n"
     ]
    }
   ],
   "source": [
    "# 获取预测所需的电池数据集\n",
    "def getBattery008data(discharge_matrix):\n",
    "    x_pred=[]\n",
    "    flag=0\n",
    "\n",
    "    # 获取得到x_pred的数据集\n",
    "    for i in range(len(discharge_matrix.tolist())):\n",
    "        #首先截取0--37这段的长度，叠加+1循环得到x\n",
    "        bataery_data_training=discharge_matrix[flag:flag+8,:]\n",
    "    #     print(bataery_data_training.shape)\n",
    "        if(len(bataery_data_training)<8):\n",
    "            break\n",
    "        else:\n",
    "            #进行转置，形成3*T的时间维度\n",
    "            x_pred.append(bataery_data_training.T)\n",
    "        flag+=1\n",
    "\n",
    "    x_pred=np.array(x_pred[:len(x_pred)-1])\n",
    "    return x_pred\n",
    "\n",
    "# return prediction  data &  prediction  data true_so\n",
    "discharge_008=r'/home/aqts/yangHong/battery experiment/jsonData/B0008_discharge.json'\n",
    "# 获取得到一节电池的 特征向量 与true soc \n",
    "discharge_matrix_008,true_soc_008 =batterydata2matrix(discharge_008)\n",
    "# 得到预测样本集 t=7\n",
    "x_test_008=getBattery008data(discharge_matrix_008)\n",
    "\n",
    "\n",
    "print(\"x_test_008 data shape is\",x_test_008.shape)\n",
    "# 预测数据集进行转换\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    x_test_008 = x_test_008.reshape(x_test_008.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_test_008 = x_test_008.reshape(x_test_008.shape[0], img_rows, img_cols,1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred=model.predict(x_test_008, batch_size=16, verbose=0)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd # 导入pandas库用来处理csv文件\n",
    "import matplotlib.pyplot as plt # 导入matplotlib.pyplot并用plt简称\n",
    "\n",
    "# true soc predict plot\n",
    "x_true_soc=range(len(true_soc_008))\n",
    "# 切割开始的基数一定为7\n",
    "start=8\n",
    "x_test_soc_008=range(start,start+pred.shape[0])\n",
    "print(\"x_true_soc range is \",x_true_soc,\"\\n x_test_soc_008  range is:\",x_test_soc_008)\n",
    "plt.plot(x_true_soc, true_soc_008, linewidth=2.0, color='r',label=\"True SOC\")#线的宽度为2.0，颜色为红色\n",
    "plt.plot(x_test_soc_008, pred, linewidth=2.0, color='g',label=\"Prediction SOC\")#线的宽度为2.0，颜色为绿色\n",
    "\n",
    "\n",
    "# 设置x ,y轴的步长与刻度属性\n",
    "plt.xticks([x for x in range(len(x_true_soc) + 1) if x %10 == 0])  # x标记step设置为10\n",
    "plt.ylim(ymin=0.3,ymax=1.1)\n",
    "# remark for x & y\n",
    "plt.xlabel('Cylce')\n",
    "plt.ylabel('SOC')\n",
    "plt.title('Real Soc and Predicted Soc')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()#显示图像\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
